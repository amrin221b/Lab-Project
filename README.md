# Lab-Project

This repository contains all the resources and findings of our lab project completed during my M1 , which explores the comparative performance of Convolutional Neural Networks (CNNs) and Vision Transformer (ViT) models in the domain of sketch recognition. The project was conducted by Amrin Akter and Hajar Toubali, under the guidance of Phlypo Ronald, Guyader Nathalie, and Dohen Marion at Grenoble INP - Phelma, UGA.

## Project Overview
The primary goal of our research was to explore and compare the effectiveness of CNN and ViT models in recognizing human-drawn sketches. The project included developing code, running experiments, and compiling the results into a detailed report.

### Research Question

Our research centered around the question: "How does the accuracy rate of Convolutional Neural Network models compare to Vision Transformer models in classifying objects from human-drawn sketches?"

### Methodology

- **Programming Languages and Tools:** Python, TensorFlow, PyTorch, Latex
- **Dataset:** TU-Berlin Sketch Dataset 
- **Models Evaluated:** 
        - CNN Models: AlexNet, Sketch-a-Net, ResNet-18
        - Vision Transformer Models: ViT-B/32, ViT-L/32, ViT-H/14

### Key Findings

- ViT models, particularly ViT-H/14, demonstrated superior performance in terms of accuracy and loss metrics compared to CNN models.
- The study confirmed the hypothesis that ViT models handle the global contextual understanding of sketches better than CNNs.

### N.B.

- It was a project which was supposed to do for practising different stage of research. So, there may have some terrible mistakes as we have explored soo many things for the first time